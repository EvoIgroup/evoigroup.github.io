---
date: 2023-04-13
title: "[CVPR2023]基于关键帧还原视频事件用于视频异常检测"
linkTitle: "One Paper accepted by CVPR2023"
description: "One Paper accepted by CVPR2023"
author: Zhiwei Yang(zwyang97@163.com)
Tags: ["AP-TS"]
---

# 基于关键帧还原视频事件用于视频异常检测

| Title        | Video Event Restoration Based on Keyframes for Video Anomaly Detection |
| ------------ | ------------------------------------------------------------ |
| Author       | Zhiwei Yang; Jing Liu; Zhaoyang Wu; Peng Wu; Xiaotao Liu     |
| Affiliations | Xidian University; Northwestern Polytechnical University     |
| Emails       | zwyang97@163.com (Dr. Zhiwei Yang)                           |
| Paper        | <https://arxiv.org/abs/2304.05112>                             |

## 1. 摘要

​视频异常检测是一个至关重要的计算机视觉问题。现有的基于深度神经网络的视频异常检测方法大多遵循帧重构或帧预测两种路线。然而，由于缺乏对视频中更高阶视觉特征和时序上下文关系的挖掘，限制了这两种方式在异常检测性能上的进一步提升。受视频编解码理论的启发，我们提出了一种新的视频异常检测范式来突破这些限制：基于关键帧还原视频事件用于视频异常检测。首先，我们提出了一个新的基于关键帧还原视频事件的任务，此任务通过鼓励深度神经网络基于视频的关键帧来推理视频序列中间缺失的多帧，从而还原完整的视频事件，它能促使深度神经网络来挖掘和学习视频中潜在的高阶视觉特征和全局的时序上下文关系。为此，我们提出了一个新颖的带有双跳跃连接的U型 Swin Transformer 网络（USTN-DSC）用于视频事件还原。其中设计的跨注意力和时序上采样残差跳跃连接用来进一步辅助还原视频中复杂的静态和动态运动目标特征。另外，我们还提出了一个简单有效的邻帧差分时序损失函数用于约束视频序列的时序一致性。在三个公开数据集上的大量实验结果表明，USTN-DSC的性能优于现有的大多数方法，验证了我们方法的有效性。

## 2. 动机

​现有的基于帧重构和帧预测的视频异常检测方法由于缺乏对视频中更高阶视觉特征和全局的时序上下文关系的挖掘，限制了其两者性能的进一步提升。
为了探索更好的方法来挖掘视频中复杂的时空关系从而进一步提升视频异常检测性能。我们受视频编解码理论的启发，提出了一个新的视频异常检测范式：基于关键帧还原视频事件来用于视频异常检测。在视频编解码过程中，会用到I帧、P帧和B帧三种不同类型的帧，其中，I帧包含当前帧完整的外观信息，被称之为关键帧，P帧记录着当前帧与前一帧的运动差异信息，B帧记录着当前帧与前后两帧之间的运动差异信息。I帧、P帧和B帧包含有视频序列中明确的外观和运动相对关系，基于这三种类型的帧，就能解码视频。受到这个过程的启发，我们的想法由此产生：如果我们给出包含有视频中隐式的外观和运动相对关系的关键帧，然后鼓励深度神经网络来探索和挖掘视频中潜在的时空变化关系，从而推理出其中缺失的多帧来还原视频事件，这一过程理论上应该是可行的。为了促使深度神经网络主动的去探索和学习视频中的时空演化关系，我们不提供类似包含明确运动信息的P帧和B帧作为输入。因为深度神经网络需要学会推理出视频序列中缺失的多帧，这一任务相比基于重构和基于预测的任务是相当有挑战性的。为了得到一个较好的还原效果，这需要视频序列之间具有较强的规律性和时序关联性。相反，对于包含有不规律和随机事件的异常视频序列，视频事件还原的结果将比较差。基于这一假设，我们提出的基于关键帧还原视频事件的任务能很好的应用于视频异常检测问题中。下图比较了视频编解码和视频事件还原任务的区别。

{{< imgproc video codec Fill "687x351" >}}
video codec
{{< /imgproc >}}

## 3. 创新点

我们介绍了一种新的视频异常检测范式：基于关键帧还原视频事件来检测异常。它能更促使深度神经网络更加有效的挖掘和学习视频中的高阶视觉特征和全局时序上下文关系。

我们提出了一个新颖的网络模型USTN-DSC用于视频事件还原。其中设计的跨注意力和时序上采样残差跳跃连接用来进一步辅助还原视频中复杂的静态和动态运动目标特征。

我们提出了一种简单有效的相邻帧差损失函数用于约束所还原视频序列和真实视频序列的运动一致性。

## 4. 方法

​为了执行这个具有挑战的视频事件还原任务，我们提出了一个新颖的带有双跳跃连接的U型 Swin Transformer 网络（USTN-DSC）用于视频事件还原。USTN-DSC遵循着经典的U-Net网络架构设计形式，它主要由特征提取器、编码器、解码器和输出头四个部分组成。其中特征提取器和输出头主要由2D卷积层组成，编码器和解码器则由多层swin transformer 块和2D卷积层联合组成。下图展示了USTN-DSC的总体架构图。

{{< imgproc main Fill "1357x998" >}}
main
{{< /imgproc >}}

​给定一个包含T帧的视频序列，我们取其中的起始帧、中间帧和结束帧作为输入的三个关键帧。这三个关键帧在时序维度上堆叠，然后输入特征提取器进行初始的特征提取和维度缩减。接着，再将其输入编码器进行特征编码，得到高阶特征表示，再将其输入解码器进行解码。在解码部分，为了应对视频中复杂的运动模式从而更好的还原视频事件，我们在USTN-DSC的编解码器中构建了双跳跃连接。首先，我们在构成编码器和解码器的swin transformer块中的基于常规和偏移窗口的多头自注意力模块后分别加入了一个跨注意力机制模块。这个模块接收来自前一个解码层的输出特征作为Query，接收来自编码器对应层级输出的特征作为Key和Value。通过查询编码器对应层级输出的不同尺度和距离的特征，跨注意力机制模块能辅助解码器更好的生成缺失帧中特定快速运动目标的特征。另外，我们还设计了一个由3D反卷积层构成的时序上采样模块，以残差的形式连接编解码器对应的各个编解码层。这个模块可以弥补跨注意力连接中原始细节特征查询的不足，从而进一步促进解码器更好地恢复视频序列中的背景和慢速目标的细节信息。最后，解码器的输出通过输出头得到还原后的视频序列。

​在测试阶段，通过计算还原后的视频序列与真实序列之间的PSNR，然后通过归一化得到对应的异常分数用于检测是否发生异常。具体公式如下：

{{< imgproc PSNR Fill "590x138" >}}
PSNR
{{< /imgproc >}}

{{< imgproc PSNR_norm Fill "605x76" >}}
PSNR_norm
{{< /imgproc >}}

## 5. 损失函数

​我们的损失函数主要包括外观损失和运动损失两部分。外观损失函数我们采用Charbonnier损失，它弥补了L1和L2损失函数的缺点，具体公式如下：

{{< imgproc loss1 Fill "356x75" >}}
loss1
{{< /imgproc >}}

​为了确保还原的视频序列与真实视频序列的时序变化保持一致，我们提出了一个简单有效的相邻帧差损失来进行时序约束，具体公式如下：

{{< imgproc loss2 Fill "647x166" >}}
loss2
{{< /imgproc >}}

## 6. 实验结果

​我们在三个公开数据集，Ped2、Avenue和ShanghaiTech上进行了实验。我们采用帧级别的AUC作为评价指标。下表是与现有方法的比较结果：

{{< imgproc SOTA Fill "673x938" >}}
SOTA
{{< /imgproc >}}

​为了更直观的表明我们方法的效果，我们展示了一些定性的结果

{{< imgproc video events restoration Fill "1347x934" >}}
video events restoration
{{< /imgproc >}}

​这个图展示了我们方法在三个数据集上基于异常视频的关键帧还原视频事件的结果。第一行为真实视频事件序列，第二行为我们方法基于关键帧还原的结果，第三行为对应还原误差的可视化图。带有红框的为对应的关键帧。

{{< imgproc compare with renc and pred Fill "658x210" >}}
compare with renc and pred
{{< /imgproc >}}

​这个图比较了USTN-DSC与基于重构和基于预测方法对于异常样本的输出结果。

{{< imgproc anomaly score Fill "988x288" >}}
anomaly score
{{< /imgproc >}}

这个图展示我们方法在三个数据集上部分测试样本上的异常分数曲线图。

## 7. 消融实验

​为了进一步表明我们方法各个组件的有效性以及网络超参数和损失函数对方法性能的影响，我们进行了大量的消融实验，实验结果如下表所示。

{{< imgproc table2 Fill "662x334" >}}
table2
{{< /imgproc >}}

{{< imgproc table3 Fill "652x236" >}}
table3
{{< /imgproc >}}

{{< imgproc table4 Fill "659x245" >}}
table4
{{< /imgproc >}}

{{< imgproc table1_suppl Fill "647x164" >}}
table1_suppl
{{< /imgproc >}}

​为了更直观的展示所设计的双跳跃连接在还原视频事件中各自所发挥的作用，我们对两个跳跃连接在不同编解码层所对应的注意力和特征映射图进行了可视化，如下图所示。

{{< imgproc Fig1_suppl Fill "1332x840" >}}
Fig1_suppl
{{< /imgproc >}}

{{< imgproc Fig2_suppl Fill "1323x811" >}}
Fig2_suppl
{{< /imgproc >}}

​从上面两图中可以看出，跨注意力连接主要负责前景动态目标特征的传递和转换。而时序上采样连接则主要负责背景静态目标特征的传递和转换。此外，我们还可以发现，在不同的解码阶段，跨注意力连接和时序上采样残差连接分别负责于不同的前景和背景部分，两者相互补充。

## 8. 总结

​在这项工作中，我们介绍了一种新的基于关键帧还原视频事件用于视频异常检测的范式。为此，我们提出了一个新颖的网络模型USTN-DSC用于视频事件还原。在USTN-DSC中，我们设计了一个跨注意力和一个时序上采样残差跳跃连接来进一步辅助还原视频序列中复杂的动态和静态运动目标特征。另外，为了确保还原的视频序列与真实视频序列的时序变化保持一致，我们还提出了一个简单有效的相邻帧差损失来进行时序约束。在三个公开数据集上的大量实验表明了我们方法的性能超越了现有的大多数方法，证明了我们方法的有效性。
