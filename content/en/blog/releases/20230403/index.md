---
date: 2023-04-03
title: "A QUICK VIEW OF TWO PAPERS ON LMAAS"
linkTitle: "A QUICK VIEW OF TWO PAPERS ON LMAAS"
description: "A QUICK VIEW OF TWO PAPERS ON LMAAS"
author: Yujian Li
Tags: ["ML-AL"]
---

DISCLAIMER: When you are reading, I could neither completely confirm nor completely deny details of any personal point of view, prejudice or mistake in this passage, including this sentence, since the mind of mine is always changing so much that even PPO could not be implemented to train myself because of the limitation of the importance sampling, i.e. the person who wrote this passage is definitely not the person who I am when you're viewing this sentence.

## Black-Box Tuning for Language-Model-as-a-Service (ICML 2022)

|    Title     | Black-Box Tuning for Language-Model-as-a-Service             |
| :----------: | ------------------------------------------------------------ |
|    Author    | Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu |
| Affiliations | Fudan University, East China Normal University, Peng Cheng Laboratory |
|    Paper     | https://arxiv.org/abs/2201.03514                             |

{{< imgproc 1 Fill "1842x942" >}}
Black-Box Tuning for Language-Model-as-a-Service (ICML 2022)
{{< /imgproc >}}

1. METHOD
initialize a prompt and optimize the increment of it. For all tasks but sentence-pair tasks, randomly sample n tokens from the PTM vocabulary as an initial prompt. For sentence-pair tasks, pretrain a prompt embedding on some publicly available NLI tasks for a better initialization. Initializing a prompt with random and countinous tunings -> the explainablity problem and the unefficient problem? It is not really practical because the server still needs to be adapted.

optimize z in a much smaller subspace (d â‰ª D) since large-scale PTMs have a low intrinsic dimensionality. A random projection matrix A, sampled from a uniform distribution (mean value is 0 while the std dev is the std dev of the hidden states of each layer after clipping outliers), projects z (d dimension) on the original prompt space P (D dimension). A random projection? Fine..... The reason is the side effect of the direct optimization on the prompt tokens by the evolutioary algorithmn.
consider two loss functions: cross entropy and hinge loss. The logits of the ouput are needed.

black box tuning - CMA-ES: Covariance Matrix Adaptation Evolution Strategy, which is to sample a new population from the normal distribution whose parameters, such as the mean vector, the overall standard deviation and the covariance matrix, are optimized in the evolution process. No adaptations and customizations to generate better offsprings exist here.

2. EXPERIMENTS

datasets. Several common language understanding classification tasks. Classification tasks are transferred into cloze tasks.Similar to Uni-Perceiver, any task is available as long as it could be considered as a text-to-text problem. But, obviously, postprocess is needed to map the texts to the answers.

few-shot setting. Randomly select k=16 samples for each class to construct a k-shot training set, and compose a development set by randomly drawing another k samples from the original training set. Use the original development sets as the test sets. For datasets without development sets, use the original test sets.

backbone models - RoBERTa(LARGE) since it could have a very small intrinsic dimensionality (about hundreds) on many tasks.

hyperparameters. Defaultly, subspace dimension d is 500, population size is 20, prompt length is 50 and loss function is cross entropy.

## Black-box Prompt Learning for Pre-trained Language Models (TMLR 2023)

|    Title     | Black-box Prompt Learning for Pre-trained Language Models    |
| :----------: | ------------------------------------------------------------ |
|    Author    | Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, Tong Zhang |
| Affiliations | The Hong Kong University of Science and Technology, University of California |
|    Paper     | https://arxiv.org/abs/2201.08531                             |

{{< imgproc 2 Fill "1602x770" >}}
Black-box Prompt Learning for Pre-trained Language Models (TMLR 2023)
{{< /imgproc >}}

1. METHOD
directly and independently select tokens one by one from a vocabulary to generate the prompt. The realtionship of tokens is not considered. And the generated prompts are not explainable and understandable by us stupid humans.

the vocabulary is generated by dividing each sentence in the training set by a threshold of pointwise mutual information (PMI) into good collocation pairs and selecting the ones with highest frequencies as the final candidate vocabulary. It's weird since good prompts may not exist in the texts of training set. Maybe it's just a compromise to get a finite vocabulary.

loss is similar.

optimize the probabilities of every discrete text prompt token on every position.

black box tuning - VR-PGE. Variance-reduced policy gradient estimator: get the policy gradient avoiding from the high variance which makes it challenging to converge in practice.

2. EXPERIMENTS

similarly, 16-shot. Larger numbers are tested in the ablation study.

the length of the prompt is 50, and the size of the vocabulary is 100.

backbone models --- the GPT-3 serie: GPT-3-Ada, GPT-3-Babbage, GPT-3- Curie, and GPT-3-Davinci. A completely practical implementation using openai's apis.
  
Since the large pretrained models, which are shining and burning these days, show magical capability and generalization for now, it seems that the only thing we need is alignment such as fine-tuning and prompt-tuning. However, many limitaions, such as hallucination, are considered as deadly barriers. Whether this paradigm is 42 and where is the next are hot topics everywhere. However, in my opinions, there is no need to flurry or worry since nothing is certained or unchanged, not to mention this new birth. Just like everything else, follow the heart to get one belief and go for it untill it's completely wrong.

</center>even mountains over time must get redefined</center>